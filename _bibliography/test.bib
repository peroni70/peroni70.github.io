
@misc{arik_tabnet_2020,
	title = {{TabNet}: {Attentive} {Interpretable} {Tabular} {Learning}},
	shorttitle = {{TabNet}},
	url = {http://arxiv.org/abs/1908.07442},
	doi = {10.48550/arXiv.1908.07442},
	abstract = {We propose a novel high-performance and interpretable canonical deep tabular data learning architecture, TabNet. TabNet uses sequential attention to choose which features to reason from at each decision step, enabling interpretability and more efficient learning as the learning capacity is used for the most salient features. We demonstrate that TabNet outperforms other neural network and decision tree variants on a wide range of non-performance-saturated tabular datasets and yields interpretable feature attributions plus insights into the global model behavior. Finally, for the first time to our knowledge, we demonstrate self-supervised learning for tabular data, significantly improving performance with unsupervised representation learning when unlabeled data is abundant.},
	urldate = {2024-05-20},
	publisher = {arXiv},
	author = {Arik, Sercan O. and Pfister, Tomas},
	month = dec,
	year = {2020},
	note = {arXiv:1908.07442 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/matthewperoni/Zotero/storage/YZMNUVYE/Arik and Pfister - 2020 - TabNet Attentive Interpretable Tabular Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/matthewperoni/Zotero/storage/ELKM6X9N/1908.html:text/html},
}

@article{novi_machine_2022,
	title = {Machine learning prediction of connectivity, biodiversity and resilience in the {Coral} {Triangle}},
	volume = {5},
	copyright = {2022 The Author(s)},
	issn = {2399-3642},
	url = {https://www.nature.com/articles/s42003-022-04330-8},
	doi = {10.1038/s42003-022-04330-8},
	abstract = {Even optimistic climate scenarios predict catastrophic consequences for coral reef ecosystems by 2100. Understanding how reef connectivity, biodiversity and resilience are shaped by climate variability would improve chances to establish sustainable management practices. In this regard, ecoregionalization and connectivity are pivotal to designating effective marine protected areas. Here, machine learning algorithms and physical intuition are applied to sea surface temperature anomaly data over a twenty-four-year period to extract ecoregions and assess connectivity and bleaching recovery potential in the Coral Triangle and surrounding oceans. Furthermore, the impacts of the El Niño Southern Oscillation (ENSO) on biodiversity and resilience are quantified. We find that resilience is higher for reefs north of the Equator and that the extraordinary biodiversity of the Coral Triangle is dynamic in time and space, and benefits from ENSO. The large-scale exchange of genetic material is enhanced between the Indian Ocean and the Coral Triangle during La Niña years, and between the Coral Triangle and the central Pacific in neutral conditions. Through machine learning the outstanding biodiversity of the Coral Triangle, its evolution and the increase of species richness are contextualized through geological times, while offering new hope for monitoring its future.},
	language = {en},
	number = {1},
	urldate = {2024-08-01},
	journal = {Communications Biology},
	author = {Novi, Lyuba and Bracco, Annalisa},
	month = dec,
	year = {2022},
	note = {Publisher: Nature Publishing Group},
	keywords = {Biodiversity, Biogeography, Coral reefs},
	pages = {1--8},
	file = {Full Text PDF:/Users/matthewperoni/Zotero/storage/LNJWWMTN/Novi and Bracco - 2022 - Machine learning prediction of connectivity, biodi.pdf:application/pdf},
}

@inproceedings{andrew_deep_2013,
	title = {Deep {Canonical} {Correlation} {Analysis}},
	url = {https://proceedings.mlr.press/v28/andrew13.html},
	abstract = {We introduce Deep Canonical Correlation Analysis (DCCA), a method to learn complex nonlinear transformations of two views of data such that the resulting representations are highly linearly correlated. Parameters of both transformations are jointly learned to maximize the (regularized) total correlation.   It can be viewed as a nonlinear extension of the linear method {\textbackslash}emphcanonical correlation analysis (CCA).  It is an alternative to the nonparametric method {\textbackslash}emphkernel canonical correlation analysis (KCCA) for learning correlated nonlinear transformations. Unlike KCCA, DCCA does not require an inner product, and has the advantages of a parametric method: training time scales well with data size and the training data need not be referenced when computing the representations of unseen instances.  In experiments on two real-world datasets, we find that DCCA learns representations with significantly higher correlation than those learned by CCA and KCCA. We also introduce a novel non-saturating sigmoid function based on the cube root that may be useful more generally in feedforward neural networks.},
	language = {en},
	urldate = {2024-10-23},
	booktitle = {Proceedings of the 30th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Andrew, Galen and Arora, Raman and Bilmes, Jeff and Livescu, Karen},
	month = may,
	year = {2013},
	note = {ISSN: 1938-7228},
	pages = {1247--1255},
	file = {Full Text PDF:/Users/matthewperoni/Zotero/storage/XNRXVPXD/Andrew et al. - 2013 - Deep Canonical Correlation Analysis.pdf:application/pdf},
}

@misc{zhang_why_2024,
	title = {Why {Transformers} {Need} {Adam}: {A} {Hessian} {Perspective}},
	shorttitle = {Why {Transformers} {Need} {Adam}},
	url = {http://arxiv.org/abs/2402.16788},
	doi = {10.48550/arXiv.2402.16788},
	abstract = {SGD performs worse than Adam by a significant margin on Transformers, but the reason remains unclear. In this work, we provide an explanation through the lens of Hessian: (i) Transformers are "heterogeneous": the Hessian spectrum across parameter blocks vary dramatically, a phenomenon we call "block heterogeneity"; (ii) Heterogeneity hampers SGD: SGD performs worse than Adam on problems with block heterogeneity. To validate (i) and (ii), we check various Transformers, CNNs, MLPs, and quadratic problems, and find that SGD can perform on par with Adam on problems without block heterogeneity, but performs worse than Adam when the heterogeneity exists. Our initial theoretical analysis indicates that SGD performs worse because it applies one single learning rate to all blocks, which cannot handle the heterogeneity among blocks. This limitation could be ameliorated if we use coordinate-wise learning rates, as designed in Adam.},
	urldate = {2024-10-23},
	publisher = {arXiv},
	author = {Zhang, Yushun and Chen, Congliang and Ding, Tian and Li, Ziniu and Sun, Ruoyu and Luo, Zhi-Quan},
	month = oct,
	year = {2024},
	note = {arXiv:2402.16788},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:/Users/matthewperoni/Zotero/storage/GT79CCE2/Zhang et al. - 2024 - Why Transformers Need Adam A Hessian Perspective.pdf:application/pdf;Snapshot:/Users/matthewperoni/Zotero/storage/GXW95X8K/2402.html:text/html},
}

@misc{ghalebikesabi_differentially_2023,
	title = {Differentially {Private} {Diffusion} {Models} {Generate} {Useful} {Synthetic} {Images}},
	url = {http://arxiv.org/abs/2302.13861},
	doi = {10.48550/arXiv.2302.13861},
	abstract = {The ability to generate privacy-preserving synthetic versions of sensitive image datasets could unlock numerous ML applications currently constrained by data availability. Due to their astonishing image generation quality, diffusion models are a prime candidate for generating high-quality synthetic data. However, recent studies have found that, by default, the outputs of some diffusion models do not preserve training data privacy. By privately fine-tuning ImageNet pre-trained diffusion models with more than 80M parameters, we obtain SOTA results on CIFAR-10 and Camelyon17 in terms of both FID and the accuracy of downstream classifiers trained on synthetic data. We decrease the SOTA FID on CIFAR-10 from 26.2 to 9.8, and increase the accuracy from 51.0\% to 88.0\%. On synthetic data from Camelyon17, we achieve a downstream accuracy of 91.1\% which is close to the SOTA of 96.5\% when training on the real data. We leverage the ability of generative models to create infinite amounts of data to maximise the downstream prediction performance, and further show how to use synthetic data for hyperparameter tuning. Our results demonstrate that diffusion models fine-tuned with differential privacy can produce useful and provably private synthetic data, even in applications with significant distribution shift between the pre-training and fine-tuning distributions.},
	urldate = {2024-11-04},
	publisher = {arXiv},
	author = {Ghalebikesabi, Sahra and Berrada, Leonard and Gowal, Sven and Ktena, Ira and Stanforth, Robert and Hayes, Jamie and De, Soham and Smith, Samuel L. and Wiles, Olivia and Balle, Borja},
	month = feb,
	year = {2023},
	note = {arXiv:2302.13861},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Cryptography and Security},
	file = {Preprint PDF:/Users/matthewperoni/Zotero/storage/GU748VP8/Ghalebikesabi et al. - 2023 - Differentially Private Diffusion Models Generate U.pdf:application/pdf;Snapshot:/Users/matthewperoni/Zotero/storage/QSS7QT57/2302.html:text/html},
}

@article{torfi_differentially_2022,
	title = {Differentially private synthetic medical data generation using convolutional {GANs}},
	volume = {586},
	issn = {0020-0255},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025521012391},
	doi = {10.1016/j.ins.2021.12.018},
	abstract = {Deep learning models have demonstrated superior performance in several real-world application problems such as image classification and speech processing. However, creating these models in sensitive domains like healthcare typically requires addressing certain privacy challenges that bring unique concerns. One effective way to handle such private data concerns is to generate realistic synthetic data that can provide practically acceptable data quality as well as be used to improve model performance. To tackle this challenge, we develop a differentially private framework for synthetic data generation using Rényi differential privacy. Our approach builds on convolutional autoencoders and convolutional generative adversarial networks to preserve critical characteristics of the generated synthetic data. In addition, our model can capture the temporal information and feature correlations present in the original data. We demonstrate that our model outperforms existing state-of-the-art models under the same privacy budget using several publicly available benchmark medical datasets in both supervised and unsupervised settings. The source code of this work is available at https://github.com/astorfi/differentially-private-cgan.},
	urldate = {2024-11-04},
	journal = {Information Sciences},
	author = {Torfi, Amirsina and Fox, Edward A. and Reddy, Chandan K.},
	month = mar,
	year = {2022},
	keywords = {Deep learning, differential privacy, generative adversarial networks, synthetic data generation},
	pages = {485--500},
	file = {ScienceDirect Snapshot:/Users/matthewperoni/Zotero/storage/I7XLXHA4/S0020025521012391.html:text/html;Submitted Version:/Users/matthewperoni/Zotero/storage/NGZSEXJF/Torfi et al. - 2022 - Differentially private synthetic medical data gene.pdf:application/pdf},
}

@article{shibata_practical_2024,
	title = {Practical {Medical} {Image} {Generation} with {Provable} {Privacy} {Protection} {Based} on {Denoising} {Diffusion} {Probabilistic} {Models} for {High}-{Resolution} {Volumetric} {Images}},
	volume = {14},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/14/8/3489},
	doi = {10.3390/app14083489},
	abstract = {Local differential privacy algorithms combined with deep generative models can enhance secure medical image sharing among researchers in the public domain without central administrators; however, these images were limited to the generation of low-resolution images, which are very insufficient for diagnosis by medical doctors. To enhance the performance of deep generative models so that they can generate high-resolution medical images, we propose a large-scale diffusion model that can, for the first time, unconditionally generate high-resolution (256×256×256) volumetric medical images (head magnetic resonance images). This diffusion model has 19 billion parameters, but to make it easy to train it, we temporally divided the model into 200 submodels, each of which has 95 million parameters. Moreover, on the basis of this new diffusion model, we propose another formulation of image anonymization with which the processed images can satisfy provable Gaussian local differential privacy and with which we can generate images semantically different from the original image but belonging to the same class. We believe that the formulation of this new diffusion model and the implementation of local differential privacy algorithms combined with the diffusion models can contribute to the secure sharing of practical images upstream of data processing.},
	language = {en},
	number = {8},
	urldate = {2024-11-04},
	journal = {Applied Sciences},
	author = {Shibata, Hisaichi and Hanaoka, Shouhei and Nakao, Takahiro and Kikuchi, Tomohiro and Nakamura, Yuta and Nomura, Yukihiro and Yoshikawa, Takeharu and Abe, Osamu},
	month = jan,
	year = {2024},
	note = {Number: 8
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {differential privacy, deep generative models, denoising, diffusion models, head magnetic resonance images},
	pages = {3489},
	file = {Full Text PDF:/Users/matthewperoni/Zotero/storage/BZQDZQUF/Shibata et al. - 2024 - Practical Medical Image Generation with Provable P.pdf:application/pdf},
}

@inproceedings{song_stochastic_2013,
	title = {Stochastic gradient descent with differentially private updates},
	url = {https://ieeexplore.ieee.org/abstract/document/6736861},
	doi = {10.1109/GlobalSIP.2013.6736861},
	abstract = {Differential privacy is a recent framework for computation on sensitive data, which has shown considerable promise in the regime of large datasets. Stochastic gradient methods are a popular approach for learning in the data-rich regime because they are computationally tractable and scalable. In this paper, we derive differentially private versions of stochastic gradient descent, and test them empirically. Our results show that standard SGD experiences high variability due to differential privacy, but a moderate increase in the batch size can improve performance significantly.},
	urldate = {2024-11-20},
	booktitle = {2013 {IEEE} {Global} {Conference} on {Signal} and {Information} {Processing}},
	author = {Song, Shuang and Chaudhuri, Kamalika and Sarwate, Anand D.},
	month = dec,
	year = {2013},
	keywords = {Algorithm design and analysis, Data privacy, Linear programming, Logistics, Noise, Privacy, Signal processing algorithms},
	pages = {245--248},
	file = {Full Text PDF:/Users/matthewperoni/Zotero/storage/ZJJJTQYZ/Song et al. - 2013 - Stochastic gradient descent with differentially pr.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/matthewperoni/Zotero/storage/F32ZLS4L/6736861.html:text/html},
}
